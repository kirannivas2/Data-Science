{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'\n",
    "import ssl\n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urllib.request.urlopen(url, context=ssl.SSLContext(), timeout=5) as response:\n",
    "    soup = BeautifulSoup(response.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_a_tags_with_href = set(filter(lambda x: 'href' in x.attrs, soup.body.find_all('a')))\n",
    "\n",
    "links_in_page = set(map(lambda x: x.attrs['href'], all_a_tags_with_href))\n",
    "\n",
    "links_in_page = set(map(lambda x: urllib.parse.urljoin(url, x), links_in_page))\n",
    "len(links_in_page)\n",
    "\n",
    "links_in_wikipedia = set(filter(lambda x: urllib.parse.urlparse(x).netloc.endswith('wikipedia.org'), links_in_page))\n",
    "len(links_in_wikipedia)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages_on_wikipedia = set()\n",
    "\n",
    "for wiki_url in links_in_wikipedia:\n",
    "    try:\n",
    "        with urllib.request.urlopen(wiki_url, context=ssl.SSLContext(), timeout=5) as response:\n",
    "            content_type = response.info()['Content-Type'] \n",
    "            if 'text/html' in content_type:\n",
    "                webpages_on_wikipedia.add(wiki_url)\n",
    "    except:\n",
    "        print(f'Unable to open {wiki_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open https://my.wikipedia.org/wiki/%E1%80%A1%E1%80%91%E1%80%B0%E1%80%B8:%E1%80%98%E1%80%9A%E1%80%BA%E1%80%80%E1%80%9C%E1%80%84%E1%80%B7%E1%80%BA%E1%80%91%E1%80%AC%E1%80%B8%E1%80%9C%E1%80%B2/%E1%80%A1%E1%80%81%E1%80%BB%E1%80%80%E1%80%BA%E1%80%A1%E1%80%9C%E1%80%80%E1%80%BA%E1%80%9E%E1%80%AD%E1%80%95%E1%80%B9%E1%80%95%E1%80%B6%E1%80%95%E1%80%8A%E1%80%AC\n",
      "Unable to open https://fa.wikipedia.org/wiki/ویکی‌پدیا:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n"
     ]
    }
   ],
   "source": [
    "col_names = ['source_url', 'link_url', 'link_title']\n",
    "df = pd.DataFrame(columns = col_names)\n",
    "\n",
    "links_in_wikipedia = webpages_on_wikipedia\n",
    "\n",
    "i = -1\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "while len(df)<100 and i<100:\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "    j = 0\n",
    "    \n",
    "    url = list(links_in_wikipedia)[i]\n",
    "\n",
    "    with urllib.request.urlopen(url, context=ssl.SSLContext(), timeout=5) as response:\n",
    "        soup = BeautifulSoup(response.read())\n",
    "\n",
    "    all_a_tags_with_href = set(filter(lambda x: 'href' in x.attrs, soup.body.find_all('a')))\n",
    "\n",
    "    links_in_page = set(map(lambda x: x.attrs['href'], all_a_tags_with_href))\n",
    "\n",
    "    links_in_page = set(map(lambda x: urllib.parse.urljoin(url, x), links_in_page))\n",
    "\n",
    "    links_in_link = set(filter(lambda x: urllib.parse.urlparse(x).netloc.endswith('wikipedia.org'), links_in_page))\n",
    "\n",
    "\n",
    "    if len(links_in_link) >9:\n",
    "        \n",
    "        while j<10:\n",
    "    \n",
    "            wiki_url = list(links_in_link)[j]\n",
    "\n",
    "            try:\n",
    "                with urllib.request.urlopen(wiki_url, context=ssl.SSLContext(), timeout=5) as response:\n",
    "                    content_type = response.info()['Content-Type'] \n",
    "                    if 'text/html' in content_type:\n",
    "                        soup = BeautifulSoup(response.read())\n",
    "            except:\n",
    "                print(f'Unable to open {wiki_url}')\n",
    "\n",
    "            df_dict = {'source_url': list(links_in_wikipedia)[i], 'link_url': list(links_in_link)[j] , 'link_title':soup.title.text}\n",
    "            df = df.append(df_dict, ignore_index=True)\n",
    "            j = j+1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('webcrawler.csv', sep = ',', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b5397936092522ea1892bdf9454bfce39cdabc455c90e81a765b5685da224d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('CSE801')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
