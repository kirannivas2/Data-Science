{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab15: Clustering World Bank Data\n",
    "\n",
    "In this lab you will be clustering World Bank observations of per capita GDP and the literacy rate of different countries.\n",
    "\n",
    "We're going to see if we can cluster countries by literacy rates and GDP.  \n",
    "\n",
    "We'll try a number of different clustering algorithms to test clustering to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # We'll do some more imports as we go along\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# You will have to import more modules as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the files we are going to load initially\n",
    "fn_gdp_data = 'pcgdp_data.csv'\n",
    "fn_lit_data = 'literacy_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the data sets\n",
    "\n",
    "Let's load in our data sets.  **Open them as a text file.  Note how they look**.  You'll have to make some settings changes to the regular `read_csv` to get the data to be loaded correctly.\n",
    "\n",
    "For now make two data frames, one for the literacy data and one for the gdp data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our GDP data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our population data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're interested in 2020 data only.  Merge the 2020 data into a single data frame.  You may use whatever column names you'd like, but in the end we want a data frame with three columns:\n",
    "+ Country Code (as the index)\n",
    "+ 2020 per capita GDP\n",
    "+ 2020 literacy rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge these two data frames together\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's subset the data\n",
    "\n",
    "We only want countries that have data for both 2020 per capita and 2020 literacy.  So let's exclude any rows where either data column is a nan.\n",
    "\n",
    "There are many ways of doing this.  One way is to use Boolean vector operations to find just those rows that don't have NaN values in them for either the per capita GDP or literacy rate.\n",
    "\n",
    "For example, if your data frame was called `df_merged` you can use this code (provided your column labels match):\n",
    "    \n",
    "    good_rows = ~ (df_merged['2020 PC GDP'].isna() | df_merged['2020 Literacy'].isna())\n",
    "    df_merged = df_merged[good_rows]\n",
    "\n",
    "The | operator means \"logical OR\" and the ~ operator means \"negate\" or flip trues to falses and vise versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot the data\n",
    "\n",
    "Let's do a scatter plot of our points to see what they look like.  We want the X axis to be the literacy in 2020 and the Y axis to be the GDP per capita in 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's transform the data\n",
    "\n",
    "Looking at the scatter plot you should see some clusters that might be identifiable.  But you should also take a look at the individual attributes themselves.  Look at the large range on GDP compared to literacy.  One is somewhere between 0 and 100 and the other 0 to 25k.  If we were to use raw values here, clearly the GDP would be the determining factor for clustering.\n",
    "\n",
    "We need to transform our data to make sure it is scaled correctly for clustering.  There are a number of different ways to do this.  Today we are going to use a simple transformation called the [StandardScalar](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) that is part of SciKit Learn's preprocessing package.    Follow the example in the linked documentation to standardize the data we have.  We will be operating on this transformed data for our clustering analysis.\n",
    "\n",
    "Make a second scatterplot of this transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StandardScalar to transform our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a scatterplot of the transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering our transformed data\n",
    "\n",
    "Now that we've transformed (standardized) our data, we can go about trying to cluster.  We will use each of the three methods we've learned about to accomplish this.\n",
    "\n",
    "There is no \"best\" way to cluster data.  It's an art and which method we use depends on the types of questions we are going to be asking.\n",
    "\n",
    "Here we're just going to use all the methods to demonstrate how they differ in use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "\n",
    "I would like you to run a K-Means clustering analysis.\n",
    "\n",
    "Assay over K of 2 to 8 inclusive.  You may use the default settings in scikit learn for KMeans.\n",
    "\n",
    "I want you to produce a SSE plot that shows how SEE changes as the value of K increases.  This means you will have to collect the final SEE for each K we use. (It's the attribute inertia_ which is available after you call fit.)  \n",
    "\n",
    "Then I would like you to pick what looks like (on the diagnostic plot) a reasonable K.\n",
    "\n",
    "Make a scatterplot of the transformed data colored by the cluster label.  (Once you run fit the attribute labels_ becomes available that will give you the label number.  You can pass this vector as a hue argument to scatterplot.)\n",
    "\n",
    "I'd also like you to draw the centroid centers as well, so collect that data for your \"best K\" by collecting its cluster_centers_ attribute that becomes available after you call `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assay over K and plot the best SSE as a function of K to find which K to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the \"best\" K, plot the transformed data with the \n",
    "# hue being the label color (from the labels_ attribute of the model you fit)\n",
    "# You can plot the centroids by using the values in cluster_centers_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering\n",
    "\n",
    "Agglomerative clustering builds a tree (dendogram) by successively combining points and groups of points together until a single tree/cluster is produced.  We an then examine this dendogram to figure out how many clusters we might use.\n",
    "\n",
    "First follow the instructions for Agglomerative Clustering you'll find on the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) for it.  We will set `n_clusters` after we take a look at the dendogram produced, so set it to `None` for now.  Also set `distance_threshold` to 0.\n",
    "\n",
    "Make sure you are operating on the transformed data.\n",
    "\n",
    "Import `plot_dendogram` from the `demo_tools` module included with today's exercise.  Simply pass in the AgglomerativeClustering model you fit to plot the denodgram.\n",
    "\n",
    "Which `linkage` method should you use?  What should the `distance_threshold` be in your final clustering?  Use the denodgram and adjust the linkage setting to find one that looks like it works best for this data.  (For me `single` linkage worked best.  But you do have to look at the dendogram to find a good `distance_threshold` for which ever method you choose.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full dendogram for the clusters and plot it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From your results above, you should see some natural places to cut our dendogram by examining either how many clusters we expect or setting a threshold distance, or how far away clusters should be.  Using which ever method you'd like, let's create our final agglomerative clustering.\n",
    "\n",
    "Make a scatter plot of the transformed data and use the labels found from the clustering to color the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the \"good\" clustering you found\n",
    "# as a scatterplot of the transformed data\n",
    "# with the hue being based on the labels_ found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "Our last clustering will be to use DBSCAN to perform the clustering.  Just as the previous two clustering algorithms required us to explore our results to find the a \"good\" set of parameter(s) for the clustering, we need to take a look at `eps` and `min_samples` to find good values for the clustering.  You can read the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) for examples of how to use it.\n",
    "\n",
    "But before we begin, let's take a look at all pairwise distances.  This will help us pick an `eps`.  Read the directions for `pairwise-distances` from the sklearn.metrics module.  Plot these distances using a seaborn `displot` with `kde=True` to see the curve.. You may need to call `flatten()` on the distance matrix you receive from `pairwise-distances` to get it to plot.\n",
    "\n",
    "As for `min_samples`?  What is the smallest cluster size you'll allow?  This is up to you.  Points that don't belong to any cluster are considered outliers.\n",
    "\n",
    "The purpose of the plot is to find a good `eps`, so we are looking for small values that are frequent.  These values are usually found near 0.  Examine the histogram and find a pairwise distance cut off.  It should be at a point in the graph where pairwise distances are separable (the valley between peaks in the plot). You may need to adjust the number of bins or filter your data to include only smaller values to get the resolution necessary for this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all pairwise distances\n",
    "# and make a displot to find a good value of eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster using DBSCAN with the eps and min_samples you've decided upon\n",
    "# Then make a scatterplot of the transformed data labeled by the labelings\n",
    "# DBSCAN found\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d87b78273520671e160d525fe6b224904c24d013cc3fb4c734a3a45961893a0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('cse801')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
